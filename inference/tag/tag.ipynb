{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (1.0.7)\n",
      "Requirement already satisfied: transformers in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (4.42.4)\n",
      "Requirement already satisfied: fairscale in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (0.4.13)\n",
      "Requirement already satisfied: pycocoevalcap in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (1.2)\n",
      "Requirement already satisfied: torch in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from timm) (2.3.1)\n",
      "Requirement already satisfied: torchvision in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from timm) (0.18.1)\n",
      "Requirement already satisfied: pyyaml in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from timm) (0.24.0)\n",
      "Requirement already satisfied: safetensors in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.9.1)\n",
      "Requirement already satisfied: sympy in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
      "/home/ttd/hcmai/aic2024/multimedia-retrieval/training/tag/recognize-anything\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('recognize-anything'):\n",
    "    !git clone https://github.com/xinyu1205/recognize-anything.git\n",
    "    \n",
    "!pip install timm transformers fairscale pycocoevalcap\n",
    "%cd recognize-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttd/hcmai/aic2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus, ram\n",
    "from ram import get_transform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM_PLUS weights already downloaded!\n",
      "You selected RAM\n",
      "--2024-07-27 15:38:12--  https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth\n",
      "Resolving huggingface.co (huggingface.co)... 54.230.71.28, 54.230.71.103, 54.230.71.2, ...\n",
      "Connecting to huggingface.co (huggingface.co)|54.230.71.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
      "Location: /spaces/xinyu1205/recognize-anything/resolve/main/ram_swin_large_14m.pth [following]\n",
      "--2024-07-27 15:38:13--  https://huggingface.co/spaces/xinyu1205/recognize-anything/resolve/main/ram_swin_large_14m.pth\n",
      "Reusing existing connection to huggingface.co:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/e6/78/e678f8565485a3f321b1180e4c7e1e18a89a9295028358eedffb98981b37e11a/15c729c793af28b9d107c69f85836a1356d76ea830d4714699fb62e55fcc08ed?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ram_swin_large_14m.pth%3B+filename%3D%22ram_swin_large_14m.pth%22%3B&Expires=1722328692&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjMyODY5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lNi83OC9lNjc4Zjg1NjU0ODVhM2YzMjFiMTE4MGU0YzdlMWUxOGE4OWE5Mjk1MDI4MzU4ZWVkZmZiOTg5ODFiMzdlMTFhLzE1YzcyOWM3OTNhZjI4YjlkMTA3YzY5Zjg1ODM2YTEzNTZkNzZlYTgzMGQ0NzE0Njk5ZmI2MmU1NWZjYzA4ZWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Lr1uMtDSABhSC-128S8kJfVGbgy4ZWOrvqgGQ0mUpEkHbVUEzLXP8MuZlG-5mt-b-6cDSZXewXp3m6stdhD3M1u6wxPBkamAK0WzTLznX8OLVa-3UjX83UGrnM60m8Mkv053C%7Eakaw12J3WEPJYAAA7KKsgcahlYn3NizLqFhzrHcdcMbqO1ai%7EZ-VumW-L3zyX%7EbsJfvB--DyOFqvi8f-8xOT6sS4K4nwDut0URGeD7T7ZdrfChoshhly3ZD4wlkeWGRntbQHbFtVRHdn9sNkyDLUTpjhpIPFo2kzCLXi4Rbj7-JRc4VNA0a3vbgPRxSqubgKdjLV%7E2vvQdGmeXgw__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
      "--2024-07-27 15:38:14--  https://cdn-lfs.huggingface.co/repos/e6/78/e678f8565485a3f321b1180e4c7e1e18a89a9295028358eedffb98981b37e11a/15c729c793af28b9d107c69f85836a1356d76ea830d4714699fb62e55fcc08ed?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ram_swin_large_14m.pth%3B+filename%3D%22ram_swin_large_14m.pth%22%3B&Expires=1722328692&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjMyODY5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lNi83OC9lNjc4Zjg1NjU0ODVhM2YzMjFiMTE4MGU0YzdlMWUxOGE4OWE5Mjk1MDI4MzU4ZWVkZmZiOTg5ODFiMzdlMTFhLzE1YzcyOWM3OTNhZjI4YjlkMTA3YzY5Zjg1ODM2YTEzNTZkNzZlYTgzMGQ0NzE0Njk5ZmI2MmU1NWZjYzA4ZWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Lr1uMtDSABhSC-128S8kJfVGbgy4ZWOrvqgGQ0mUpEkHbVUEzLXP8MuZlG-5mt-b-6cDSZXewXp3m6stdhD3M1u6wxPBkamAK0WzTLznX8OLVa-3UjX83UGrnM60m8Mkv053C%7Eakaw12J3WEPJYAAA7KKsgcahlYn3NizLqFhzrHcdcMbqO1ai%7EZ-VumW-L3zyX%7EbsJfvB--DyOFqvi8f-8xOT6sS4K4nwDut0URGeD7T7ZdrfChoshhly3ZD4wlkeWGRntbQHbFtVRHdn9sNkyDLUTpjhpIPFo2kzCLXi4Rbj7-JRc4VNA0a3vbgPRxSqubgKdjLV%7E2vvQdGmeXgw__&Key-Pair-Id=K3ESJI6DHPFC7\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.33.183.105, 13.33.183.73, 13.33.183.3, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.33.183.105|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5625634877 (5.2G) [binary/octet-stream]\n",
      "Saving to: ‘pretrained/ram_swin_large_14m.pth’\n",
      "\n",
      "m_swin_large_14m.pt   9%[>                   ] 522.20M  1.33MB/s    eta 33m 11s^C\n"
     ]
    }
   ],
   "source": [
    "def download_checkpoints(model, weight_path):\n",
    "    print('You selected', model)\n",
    "    if not os.path.exists('pretrained'):\n",
    "        os.makedirs('pretrained')\n",
    "\n",
    "    if model == \"RAM_PLUS\":\n",
    "        !wget https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth -O pretrained/ram_plus_swin_large_14m.pth\n",
    "    elif model == \"RAM\":\n",
    "        !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth -O pretrained/ram_swin_large_14m.pth\n",
    "    else:\n",
    "        print('Model not found')\n",
    "\n",
    "weight_path = 'pretrained/ram_plus_swin_large_14m.pth'\n",
    "weight_path1 = 'pretrained/ram_swin_large_14m.pth'\n",
    "if not os.path.exists(weight_path):\n",
    "    download_checkpoints(\"RAM_PLUS\", weight_path)\n",
    "else:\n",
    "    print(\"RAM_PLUS weights already downloaded!\")\n",
    "    \n",
    "    \n",
    "if not os.path.exists(weight_path1):\n",
    "    download_checkpoints(\"RAM\", weight_path)\n",
    "else:\n",
    "    print(\"RAM weights already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_dir = '../../Keyframes'\n",
    "all_keyframe_paths = dict()\n",
    "for part in sorted(os.listdir(keyframes_dir)):\n",
    "    data_part = part.split('_')[-1] # L01, L02 for ex\n",
    "    all_keyframe_paths[data_part] =  dict()\n",
    "    \n",
    "\n",
    "for data_part in sorted(all_keyframe_paths.keys()):\n",
    "    data_part_path = f'{keyframes_dir}/{data_part}'\n",
    "    video_dirs = sorted(os.listdir(data_part_path))\n",
    "    video_ids = [video_dir.split('_')[-1] for video_dir in video_dirs]\n",
    "    for video_id, video_dir in zip(video_ids, video_dirs):\n",
    "        keyframe_paths = sorted(glob.glob(f'{data_part_path}/{video_dir}/*.webp'))\n",
    "        all_keyframe_paths[data_part][video_id] = keyframe_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "pretrained/ram_plus_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from pretrained/ram_plus_swin_large_14m.pth\n",
      "vit: swin_l\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "transform = get_transform(image_size=384)\n",
    "model = ram_plus(pretrained='pretrained/ram_plus_swin_large_14m.pth',\n",
    "            image_size=384,\n",
    "            vit='swin_l')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "model1 = ram(pretrained='pretrained/ram_swin_large_14m.pth',\n",
    "            image_size=384,\n",
    "            vit='swin_l')\n",
    "model1.eval()\n",
    "model1 = model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/recognize-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['chair | person | interview | man | news | stand | stool | stop sign | television | video | woman'],)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_tag(model,\n",
    "                image\n",
    "                ):\n",
    "\n",
    "    image_embeds = model.image_proj(model.visual_encoder(image))\n",
    "    image_atts = torch.ones(image_embeds.size()[:-1],\n",
    "                            dtype=torch.long).to(image.device)\n",
    "\n",
    "    image_cls_embeds = image_embeds[:, 0, :]\n",
    "    image_spatial_embeds = image_embeds[:, 1:, :]\n",
    "\n",
    "    bs = image_spatial_embeds.shape[0]\n",
    "\n",
    "    des_per_class = int(model.label_embed.shape[0] / model.num_class)\n",
    "\n",
    "    image_cls_embeds = image_cls_embeds / image_cls_embeds.norm(dim=-1, keepdim=True)\n",
    "    reweight_scale = model.reweight_scale.exp()\n",
    "    logits_per_image = (reweight_scale * image_cls_embeds @ model.label_embed.t())\n",
    "    logits_per_image = logits_per_image.view(bs, -1,des_per_class)\n",
    "\n",
    "    weight_normalized = F.softmax(logits_per_image, dim=2)\n",
    "    label_embed_reweight = torch.empty(bs, model.num_class, 512).to(image.device).to(image.dtype)\n",
    "\n",
    "    for i in range(bs):\n",
    "        reshaped_value = model.label_embed.view(-1, des_per_class, 512)\n",
    "        product = weight_normalized[i].unsqueeze(-1) * reshaped_value\n",
    "        label_embed_reweight[i] = product.sum(dim=1)\n",
    "\n",
    "    label_embed = torch.nn.functional.relu(model.wordvec_proj(label_embed_reweight))\n",
    "\n",
    "    # recognized image tags using alignment decoder\n",
    "    tagging_embed = model.tagging_head(\n",
    "        encoder_embeds=label_embed,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_atts,\n",
    "        return_dict=False,\n",
    "        mode='tagging',\n",
    "    )\n",
    "\n",
    "    logits = model.fc(tagging_embed[0]).squeeze(-1)\n",
    "\n",
    "    targets = torch.where(\n",
    "        torch.sigmoid(logits) > model.class_threshold.to(image.device),\n",
    "        torch.tensor(1.0).to(image.device),\n",
    "        torch.zeros(model.num_class).to(image.device))\n",
    "\n",
    "    tag = targets.cpu().numpy()\n",
    "    tag[:,model.delete_tag_index] = 0\n",
    "    tag_output = []\n",
    "    tag_output_chinese = []\n",
    "    for b in range(bs):\n",
    "        index = np.argwhere(tag[b] == 1)\n",
    "        token = model.tag_list[index].squeeze(axis=1)\n",
    "        tag_output.append(' | '.join(token))\n",
    "        token_chinese = model.tag_list_chinese[index].squeeze(axis=1)\n",
    "        tag_output_chinese.append(' | '.join(token_chinese))\n",
    "\n",
    "\n",
    "    return tag_output, \n",
    "\n",
    "image_path = \"/home/ttd/hcmai/aic2024/multimedia-retrieval/training/Keyframes/L02/V001/0002.webp\"\n",
    "\n",
    "\n",
    "generate_tag(model1, transform(Image.open(image_path)).unsqueeze(0).to(device))\n",
    "generate_tag(model, transform(Image.open(image_path)).unsqueeze(0).to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
